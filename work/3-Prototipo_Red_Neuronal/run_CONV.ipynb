{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dA1bbUva-y83"
   },
   "source": [
    "En VSCode! Si no funciona por ejemplo dice \"requires pip & notebook package\" ir a extensiones --> buscar Python Enviroment Manager --> Instalar --> al final de la barra lateral aparecerá el icono de python, buscan geofluidos2 y seleccionan el pulgar arriba (set as active workspace interpreter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cxHZUPz-y9G"
   },
   "source": [
    "# **Redes Neuronales Convolucionales (CNN)**\n",
    "## **<font color='lightblue'>Repaso corto de Redes Neuronales Convolucionales.</font>**\n",
    "\n",
    "### **<font color='lightblue'>Perceptrón vs Filtro Convolucional?</font>**\n",
    "\n",
    " El perceptrón recibe múltiples señales de entrada (x). Si la suma de las señales supera un umbral determinado, se produce una señal o, por el contrario, no se emite ningún resultado(f). Por lo que un perceptrón se puede relacionar como:\n",
    "\n",
    "$$\n",
    "out = f(\\sum^n w_n * x_n + b)\n",
    "$$\n",
    "\n",
    "Notemos que el perceptrón para dar un valor de salida, requiere información de todos los valores que componen la entrada. En cambio un filtro convolucional, que es lo que distingue, procesa información de forma local, es decir, posicionado en un punto central captura señales del entorno de dicho punto.\n",
    "\n",
    "El kernel, como ya vimos en el caso 1D, es un vector de pesos (o una función de densidad) que recorre una función objetivo, y en ese caso tratabamos de ajustar los pesos de ese kernel, para realizar una estimación de la función objetivo a partir del entorno punto a punto. En este caso, usaremos kernels 2D que es la misma idea pero sobre matrices. Recordemos también que la forma de operar era con un producto escalar elemento a elemento, y en este caso también lo es. También como el caso de un perceptrón las salidas de los filtros convolucionales pueden tener una función de activación, y entonces dicha salida se puede escribir como: \n",
    "\n",
    "$$\n",
    "out = f(\\sum^n w_n * x_n + b)\n",
    "$$\n",
    "con n = k * l dimensiones del kernel de (k,l).\n",
    "\n",
    "\n",
    "Vaya novedad...\n",
    "\n",
    "A su vez se dice que el filtro convolucional además de extraer información local, es un invariante traslacional, ya que el mismo filtro recorre y aplica a toda la matriz (en 2D, también existen las convoluciones 3D). Y otra definición a tener en cuenta es la de capa convolucional, que se la denomina así al conjunto de filtros convolucionales que operan sobre una misma entrada. \n",
    "\n",
    "![alternative text](./images_CONV/convolucion.png)\n",
    "\n",
    "\n",
    "### **<font color='lightblue'>¿Que es una Red Neuronal Convolucional?</font>**\n",
    "De forma similar que las Redes Neuronales Artificiales, la sucesión de capas convolucionales conforman la Red Neuronal Convolucional. En la cual los filtros de las capas convolucionales iniciales aprenden patrones simples, y en profundidad patrones más complejos. Y de ahí surge el concepto de Deep Learning, que no solo aplica a Redes Convolucionales, si no que a Redes Neuronales Artificiales también.\n",
    "\n",
    "\n",
    "**<font color='lightblue'>Otra capa importante: Maxpooling<font>**\n",
    "\n",
    "Es maxpooling es un kernel que recorre la entrada al igual que el filtro convolucional, pero en este caso su objetivo es retener el valor máximo de la salida del filtro convolucional. Esta capa tiene dos grandes fortalezas:\n",
    "- Es conservador de la Varianza.\n",
    "- Es un reductor de la Dimensionalidad. \n",
    "\n",
    "![alternative text](./images_CONV/maxpooling.png)\n",
    "\n",
    "\n",
    "Les dejo una [página](https://poloclub.github.io/cnn-explainer/) para que la exploren luego, pero es un buen ejercicio verlo en acción y como operan las convoluciones internamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# construyamos un get data que nos sirva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/fernando.huaranca/datosmunin/regiones_R_025/medios.npz'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Funcion para abrir los datos .npz y extraer las variables que elegimos, y lo guardamos en un diccionario\n",
    "def get_data(path, var_input, var_target, train_ratio, val_ratio, test_ratio):\n",
    "    \n",
    "\n",
    "    #Cargamos el archivo npz\n",
    "    data_from_file  = np.load(path) \n",
    "\n",
    "    #Generamos un diccionario vacio para almacenar variables\n",
    "    Data=dict()\n",
    "\n",
    "    #Cargamos todos los datos de precipitacion de nuestro modelo\n",
    "    Input = data_from_file[var_input]\n",
    "\n",
    "    #Cargamos todos los datos de precipitacion observado\n",
    "    Target = data_from_file[var_target]\n",
    "    \n",
    "    #En este caso tanto el input como el target tiene las mismas dimensiones, por eso podemos usarlas para ambos\n",
    "    Data[\"len_total\"], Data[\"nx\"], Data[\"ny\"]  = Input.shape\n",
    "    \n",
    "    indices = range(Data[\"len_total\"]) #Con el largo del dataset cuento cuantos hay y genero un vector de indices\n",
    "\n",
    "    #Separo en los conjuntos de Entrenamiento y un conjunto que será Validacion/Testing\n",
    "    train_ids, rest_ids = train_test_split(indices, test_size=1 - train_ratio , shuffle=False )\n",
    "    #Ahora a ese conjunto restante lo divido en Validacion y testing propiamente.\n",
    "    val_ids, test_ids   = train_test_split(rest_ids, test_size=test_ratio/(test_ratio + val_ratio) , shuffle=False ) \n",
    "\n",
    "    #Guardo e imprimo por pantalla la cantidad de datos en cada conjunto\n",
    "    Data[\"len_train\"], Data[\"len_val\"], Data[\"len_test\"] = len(train_ids), len(val_ids), len(test_ids)\n",
    "    print('Training set starts at :', str( np.min( train_ids) ) , ' and ends at: ', str( np.max( train_ids ) ) )\n",
    "    print('Validation set starts at :', str( np.min( val_ids ) ) , ' and ends at: ', str( np.max( val_ids ) ) )\n",
    "    print('Testing set starts at: ', str( np.min( test_ids ) ) , ' and ends at: ', str( np.max( test_ids ) ) )\n",
    "    \n",
    "    #Seleccionamos datos para nuestro entrenamiento\n",
    "    train_x_data = Input[train_ids,:,:]\n",
    "    train_y_data = Target[train_ids,:,:]\n",
    "    \n",
    "    #Seleccionamos datos para la validacion\n",
    "    val_x_data = Input[val_ids,:,:]\n",
    "    val_y_data = Target[val_ids,:,:]\n",
    "\n",
    "    #Seleccionamos datos para el testeo\n",
    "    test_x_data = Input[test_ids,:,:]\n",
    "    test_y_data = Target[test_ids,:,:]\n",
    "\n",
    "    Data[\"xmin\"], Data[\"xmax\"] = np.append(train_x_data,val_x_data,axis=0).min() , np.append(train_x_data,val_x_data,axis=0).max()\n",
    "    Data[\"ymin\"], Data[\"ymax\"] = np.append(train_y_data,val_y_data,axis=0).min() , np.append(train_y_data,val_y_data,axis=0).max()\n",
    "    \n",
    "    return Data, train_x_data, train_y_data, val_x_data, val_y_data, test_x_data, test_y_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_name = 'pp_medios_gfs'\n",
    "Target_name = 'pp_medios_gsmap'\n",
    "Experimento = Input_name + 'vs' + Target_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set starts at : 0  and ends at:  5833\n",
      "Validation set starts at : 5834  and ends at:  6562\n",
      "Testing set starts at:  6563  and ends at:  7292\n",
      "Muestras de Train / Valid / Test:  (5834, 729, 730)\n"
     ]
    }
   ],
   "source": [
    "#Porcentaje de reparticion de los conjuntos de Train / Validation / Testing\n",
    "train_ratio = .8\n",
    "val_ratio = .1\n",
    "test_ratio = .1\n",
    "\n",
    "path = '/home/fernando.huaranca/datosmunin/regiones_R_025/medios.npz'\n",
    "\n",
    "\n",
    "# Lectura de los datos\n",
    "#Reflectividad de radar simulada\n",
    "#Tasa de precicpitacion\n",
    "Data, x_train, y_train, x_val, y_val, x_test, y_test = get_data(path=path,\n",
    "                                                                   var_input = Input_name,\n",
    "                                                                   var_target = Target_name,\n",
    "                                                                   train_ratio = train_ratio,\n",
    "                                                                   val_ratio = val_ratio,\n",
    "                                                                   test_ratio = test_ratio)\n",
    "nx, ny = Data[\"nx\"], Data[\"ny\"]\n",
    "\n",
    "print(\"Muestras de Train / Valid / Test: \",(Data[\"len_train\"],Data[\"len_val\"],Data[\"len_test\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5jF6PoF-y9M"
   },
   "source": [
    "## Importamos las librerias y funciones que necesitaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "scgY0Jpl-y9O"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import set_dataset as ds\n",
    "import verificacion as ver\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7J6RYtxa-y9R"
   },
   "source": [
    "### Fijamos la semilla\n",
    "\n",
    "Esto es importante para lograr la reproducibilidad de los experimentos que realicemos, y que no sea un factor de incertidumbre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uD8sWkvn-y9S"
   },
   "outputs": [],
   "source": [
    "def define_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "define_seed(seed=1029)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbmxwXxD-y9T"
   },
   "source": [
    "## Elegimos las variables de Entrada (Input) y Objetivo (Target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "sgh8H8iz-y9U"
   },
   "outputs": [],
   "source": [
    "Input_name =  \"sdbz\"\n",
    "Target_name = \"rain\"\n",
    "Experimento = Input_name+\"_vs_\"+ Target_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWxFS0R3-y9V"
   },
   "source": [
    "## Lectura de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "En3bgZAx-y9W"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set starts at : 0  and ends at:  1217\n",
      "Validation set starts at : 1218  and ends at:  1369\n",
      "Testing set starts at:  1370  and ends at:  1522\n",
      "Muestras de Train / Valid / Test:  (1218, 152, 153)\n"
     ]
    }
   ],
   "source": [
    "#Porcentaje de reparticion de los conjuntos de Train / Validation / Testing\n",
    "train_ratio = .8\n",
    "val_ratio = .1\n",
    "test_ratio = .1\n",
    "\n",
    "#Aplicar Data Augmentation\n",
    "Data_Aug = False\n",
    "\n",
    "# Lectura de los datos\n",
    "#Reflectividad de radar simulada\n",
    "#Tasa de precicpitacion\n",
    "Data, x_train, y_train, x_val, y_val, x_test, y_test = ds.get_data(\"./DATA/datos_WRF_nx40.npz\",\n",
    "                                                                   Input_name, Target_name, train_ratio, val_ratio, test_ratio, Data_Aug)\n",
    "nx, ny = Data[\"nx\"], Data[\"ny\"]\n",
    "\n",
    "print(\"Muestras de Train / Valid / Test: \",(Data[\"len_train\"],Data[\"len_val\"],Data[\"len_test\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_txDedXL-y9Y"
   },
   "source": [
    "## Analicemos los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "el objetivo es que la red tome de input la reflectividad y genere un campo de precipitacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_Gpn-PR-y9a"
   },
   "source": [
    "## Hiperparametros\n",
    "\n",
    "Los hiperparametros son definidos, generalmente de forma manual, y determinan el entrenamiento de los modelos. En cambio un parametro, es estimado en el entrenamiento, por ejemplos pesos y bias.\n",
    "\n",
    "Los hiperparametros con que más nos toparemos son los siguentes:\n",
    "\n",
    "- **_Batch size_**: Es el tamaño de las muestras en las que se divide el dataset (no confundir con los datasets de entrenamiento/validación y testing), que se empaquetan en un tensor durante las iteraciones de entrenamiento. EL BATCH SIZE SIEMPRE ES MENOR O IGUAL AL TAMAÑO DE LOS DATOS DEL CONJUNTO DE ENTRENAMIENTO\n",
    "\n",
    "- **_Épocas_**: Controla el número de pasadas completas por el conjunto de datos de entrenamiento. LA CANTIDAD DE EPOCAS ES MANUAL TEORICAMENTE ES INFINITO\n",
    "\n",
    "- **_Tasa de aprendizaje / Learning Rate_**: Es el factor de escala (también llamado paso de movimiento) que proporciona la cantidad de ajuste que optimiza los parámetros (pesos y bias) a partir de considerar el gradiente de la función de costo.\n",
    "\n",
    "- **_Función de costo_**: En cada paso de entrenamiento (es decir, para cada batch en cada eṕoca) se comparan la salida del modelo contra la verdad (Target), y esa comparación se cuantifica en una métrica que propongamos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "uA7YoVZ1-y9b"
   },
   "outputs": [],
   "source": [
    "#Una forma de identificar la configuración que utilizamos\n",
    "Modelo = \"Convolucion\"\n",
    "Numero_exp = 0\n",
    "\n",
    "#Hiperparametros\n",
    "batch_size= 100\n",
    "max_epochs = 60\n",
    "learning_rate = 1e-3 \n",
    "\n",
    "#El learning rate decay seria algo que ayuda a converger. A partir de una cierta epoca el learning rate\n",
    "#decay va decayendo a un ritmo de 0.1 \n",
    "lr_decay = False\n",
    "if lr_decay:\n",
    "    milestones = [45] ; gamma=0.1\n",
    "\n",
    "#Definimos la función de costo que queremos minimizar, y también el método de calculo sobre el batch.\n",
    "MSE_Loss = torch.nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tiene sentido que el learning rate tenga un orden de 10**-3 debido a tambien como es la funcion de costo. Generlamente esta estaandrizada, y tambien pensa en como seran los pesos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "YUqLsdzN-y9c"
   },
   "outputs": [],
   "source": [
    "Directorio_out = \"./salidas/\"+Modelo+\"_\"+Experimento+\"_\"+str(Numero_exp)+\"/\"\n",
    "\n",
    "if not os.path.exists(Directorio_out):\n",
    "    # Creo un nuevo directorio si no existe (para guardar las imagenes y datos)\n",
    "    os.makedirs(Directorio_out)\n",
    "    print(\"El nuevo directorio asociado a \"+ Experimento +\" \"+str(Numero_exp)+\" ha sido creado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-_vwU7J-y9c"
   },
   "source": [
    "### Definimos el modelo a utilizar\n",
    "\n",
    "Utilizaremos el **_método forward_** para la construcción de nuestra arquitectura de red neuronal. El método forward (hacia adelante) como indica su nombre se basa en pasar nuestro input por los atributos que definimos en el **_class_**, en particular en el **_método init_** que para esto sirve. Es decir, vamos a hacer pasar el input por las sucesivas capas que componen la red neuronal hasta que llegar el output, la capa final.\n",
    "\n",
    "Existe otra forma de construir la red neuronal, y es bajo la función **nn.Sequential**, que funciona igual que el método forward pero en este caso solo definimos las capas de la red neuronal, en el orden que queremos. Un ejemplo:\n",
    "\n",
    "model = nn.Sequential(\n",
    "  \\\n",
    "          nn.Linear(in_features = 16, out_features = 8),\n",
    "  \\\n",
    "          nn.ReLU(),\n",
    "  \\\n",
    "          nn.Linear(in_features = 8, out_features = 16),\n",
    "  \\\n",
    "          nn.SiLU()\n",
    "  \\\n",
    "        )\n",
    "\n",
    "Usar el nn.Sequential es más sencillo e implica menos código, pero el beneficio del método forward es que permite construir de una forma más manipulable arquitecturas más complejas. Por ejemplo, si se requiere trabajar con varios inputs \"independientes\" entre si, y se quiere que la red neuronal en las primeras capas procese esos inputs de forma separada. Más especificamente con Redes Neuronales Convolucionales, si se requiere trabajar con **bloques** (que son un combinación de capas convolucionales para extraer ciertas características o generar procesamientos especificos), usar este método es más amigable.\n",
    "\n",
    "Tips para optimizar lineas de código (válida para nn.Sequential también), se pueden componer las con capas de las red neuronal entre sí:\n",
    "\n",
    "def forward(self, x):\n",
    "\\\n",
    "&emsp; &emsp; self.activation_2(self.Linear_2(x))\n",
    "\n",
    "Lo recomendable para que sea entendible es una capa de activación compuesta con la correspondiente capa neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "LR1y85la-y9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nx = {nx} ny = {ny}\n"
     ]
    }
   ],
   "source": [
    "class Convolucional(nn.Module):\n",
    "\n",
    "    def __init__(self, nx, ny,filters): #Definimos los atributos de la clase\n",
    "        super().__init__()\n",
    "\n",
    "        #Nx serian tus entradas latitud (en nuestro ejemplo)\n",
    "        #Ny tambien tu otra entrada lo que seria longitud\n",
    "        self.nx, self.ny = nx, ny #Si se usan linears, es bueno guardalo...\n",
    "\n",
    "        #in_chanels: seria tu matriz de entrada que es 1\n",
    "        #out chanel: tu matriz de salida que es 16 salidas, una por cada kernel\n",
    "        #kernel_size: que tamaño tiene el kernel 5x5\n",
    "        #stride: pasos de iteracion en este caso se mueve un punto de reticula\n",
    "        #padding: agrega 2 columna y dos filas\n",
    "        #paddin_mode : reflect es que el borde se pone, en este caso uno similar al punto de reticula, podria ser 0\n",
    "        #bias : True es buena idea activarlo\n",
    "        self.conv_1 = nn.Conv2d(in_channels = filters[0],out_channels = filters[1], kernel_size=5 ,stride = 1, padding= 2, bias=True, padding_mode='reflect')\n",
    "        self.conv_2 = nn.Conv2d(in_channels = filters[1], out_channels = filters[2], kernel_size=5 ,stride = 1, padding= 2, bias=True, padding_mode='reflect')\n",
    "        \n",
    "        self.activation_1 = nn.ReLU()\n",
    "        self.activation_2 = nn.SiLU()\n",
    "        \n",
    "        #self.dropout_2d = nn.Dropout2d(p=0.2, inplace=False)\n",
    "        \n",
    "        #self.maxpool = nn.MaxPool2d(2)\n",
    "        \n",
    "        #self.transpconv_1 = nn.ConvTranspose2d(in_channels = filters[1],out_channels = filters[2], kernel_size=5 ,stride = 2, padding= 2, output_padding=1, bias=True)\n",
    "        \n",
    "        #self.Conv_BN_1 = torch.nn.BatchNorm2d(filters[1],affine=True) #Igual cantidad que la cantidad de filtros de salida\n",
    "        \n",
    "\n",
    "    def forward(self, x): #Ejecutamos con este método los atributos que definimos\n",
    "        x = torch.unsqueeze(x,1) # x - Shape 4D: (batch size, filtros, nx, ny)\n",
    "        \n",
    "        x = self.conv_1(x)\n",
    "        x = self.activation_1(x)\n",
    "\n",
    "        x = self.conv_2(x)\n",
    "        x = self.activation_2(x)\n",
    "\n",
    "        return torch.squeeze(x)\n",
    "    \n",
    "#una imagen de entrada en el primer filtro\n",
    "\n",
    "filters = [1,16,1]\n",
    "\n",
    "#Aca instancia la clase como modelo\n",
    "model = Convolucional(nx,ny,filters)\n",
    "model.to(device) #Cargamos en memoria, de haber disponible GPU se computa por ahí\n",
    "print(\"nx = {nx}\",\"ny = {ny}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpszNuNw-y9f"
   },
   "source": [
    "\n",
    "**<font color='green'>Preguntas</font>**\n",
    "\n",
    "- **<font color='green'>En función de la arquitectura de red neuronal a utilizar. Cuente y discuta cuentos parámetros entrenables posee la red neuronal en cuestión y compare con la cantidad de datos que se poseen.</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqweJJh4-y9g"
   },
   "source": [
    "# Configuración de la Red Neuronal\n",
    "\n",
    "## Descenso del gradiente y actualización de los parametros\n",
    "\n",
    "El gradiente de una función apunta en la dirección de máximo crecimiento de esta, y bajo esta premisa se utiliza el método de optimización conocido como **descenso del gradiente**, que como insinua su nombre está asociado a la dirección contraria al gradiente de una función objetivo, que este caso es la función de costo que queremos minimizar. La longitud del paso de actualización de los parametros de la red neuronal estaran determinados entonces por los puntos donde la función de costo pasa de decrecer a crecer (es decir mínimos), siguiendo el siguiente proceso iterativo:\n",
    "$$\n",
    "  w_n^{t+1} = w_n^{t} - \\lambda * \\frac{\\partial{Loss}}{\\partial{w_n^{t}}} \\qquad w_n^{t}: Peso \\; n-ésimo \\; en \\; paso \\; de \\; actualización \\; t, \\; \\lambda: Learning \\: Rate\n",
    "$$ (my_other_label)\n",
    "Cuando la función de costo satisface ciertas condiciones de regularidad (son del tipo C) el algoritmo converge a un mínimo local, que termina siendo un mínimo global en el caso de funciones estrictamente convexas. Por este motivo, es que es deseable que las funciones de costo que utilicemos sea convexas (-> U ), como por ejemplo el MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCypYOwf-y9g"
   },
   "source": [
    "## Otras configuraciones: Optimizador e Inicialización de pesos.\n",
    "\n",
    "Estrictamente lo siguiente que definiremos son hiperparametros también. Ya que son configuraciones que definimos en la etapa de pre-entrenamiento, y son sumamente importantes porque configuran el objetivo y forma de entrenamiento. \n",
    "\n",
    "Los pesos se actualizaran siguiendo la idea del algoritmo de descenso del gradiente, donde la función a minimizar es la **_Función de costo (Loss Function)_** que definimos.\n",
    "\n",
    "A partir del algoritmo de **_Backprogation_** se propagara el costo y se imputará sobre los parámetros (pesos y bias) de la red neuronal.\n",
    "\n",
    "El **_Optimizador_** es el método de actualización de los pesos en cada época. En particular en este caso, utilizaremos ADAM (ADAptative Moment estimation). Brevemente lo que hace es computar un learning rate para cada paramametro (la parte adaptativa del optimizador) y también guarda el decaimiento promedio de los gradientes anteriores. Esto último en las primeras épocas es donde más se hace notar este efecto, porque los gradientes suelen ser mayores/bruscos adaptandose a los datos.\n",
    "\n",
    "El \"punto de partida\" de los parametros a optimizar para la minimización de la Función de costo, está dada por la **_distribución inicial de los pesos_**. La cual es importante considerar debido a que puede colaborar a lograr una rápida convergencia de la red neuronal hacia el mínimo global de la función de costo (de encontrarse) o por el contrario atentar contra esto y requerir un mayor número de eṕocas para lograr la convergencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWwSogOX-y9i"
   },
   "source": [
    "Notas sobre inicialización de pesos: https://eva.fing.edu.uy/pluginfile.php/317369/mod_resource/content/3/c9.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es una funcion para iniciar los pesos de manera random, la magnitud de los pesos va decayendo al pasar de las capas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "YmbpG8FT-y9j"
   },
   "outputs": [],
   "source": [
    "def initialize_weights(self):\n",
    "    for m in self.modules():\n",
    "        if isinstance(m, torch.nn.Conv2d):\n",
    "            torch.nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, torch.nn.BatchNorm2d):\n",
    "            torch.nn.init.constant_(m.weight, 1)\n",
    "            torch.nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, torch.nn.Linear):\n",
    "            torch.nn.init.xavier_normal_(m.weight)\n",
    "            torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "initialize_weights(model) #Definimos los pesos iniciales con los cuales inicia el modelo antes de entrenarlo\n",
    "\n",
    "#Definimos el optimizador (descenso de gradiente)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
    "\n",
    "#Definimos el Scheduler para el Learning Rate\n",
    "if lr_decay:\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones = milestones, gamma = gamma, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UofWRcsj-y9k"
   },
   "source": [
    "# Entrenamiento de la Red Neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVQBLFSL-y9k"
   },
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "AZb8y_mv-y9l"
   },
   "outputs": [],
   "source": [
    "#Preparamos los 3 conjuntos que definimos entrenamiento, validación y testing para que ingresen al dataloader\n",
    "\n",
    "#Input matriz de 40x40x31\n",
    "#Output matriz de 40x40x31\n",
    "train_subset = ds.set_up_data(Data=Data, Input = x_train, Target = y_train)\n",
    "val_subset = ds.set_up_data(Data=Data, Input = x_val,   Target = y_val)\n",
    "test_subset = ds.set_up_data(Data=Data, Input = x_test,  Target = y_test)\n",
    "\n",
    "#Definimos los dataloaders\n",
    "#El dataloader de train tiene definido el batch_size, no asi los de validacion y test.\n",
    "#Generalmente porque en comparación son mucho más chicos.\n",
    "dataloader_train = DataLoader(train_subset, batch_size = batch_size, shuffle=True)   \n",
    "dataloader_val   = DataLoader(val_subset , batch_size=len( val_subset) )\n",
    "dataloader_test  = DataLoader(test_subset , batch_size=len( test_subset) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvsmQLnI-y9l"
   },
   "source": [
    "## Entrenando la Red Neuronal\n",
    "\n",
    "\n",
    "**<font color='red'>Teniendo en cuenta lo anterior y la función de costo definida. Observe como en cada época varia la loss para el conjunto de entrenamiento y el de validación.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "36c8HfA6-y9m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca: 1 de 60\n",
      "Loss Train:  484.18440716083234\n",
      "Loss Val:    201.32691955566406\n",
      "Epoca: 2 de 60\n",
      "Loss Train:  293.9718991793119\n",
      "Loss Val:    202.25216674804688\n",
      "Epoca: 3 de 60\n",
      "Loss Train:  286.0790053147536\n",
      "Loss Val:    201.8604736328125\n",
      "Epoca: 4 de 60\n",
      "Loss Train:  293.81775958721454\n",
      "Loss Val:    201.61033630371094\n",
      "Epoca: 5 de 60\n",
      "Loss Train:  283.8017132098858\n",
      "Loss Val:    201.44247436523438\n",
      "Epoca: 6 de 60\n",
      "Loss Train:  293.26941270094653\n",
      "Loss Val:    201.32467651367188\n",
      "Epoca: 7 de 60\n",
      "Loss Train:  283.2016801100511\n",
      "Loss Val:    201.23220825195312\n",
      "Epoca: 8 de 60\n",
      "Loss Train:  282.84958120492786\n",
      "Loss Val:    201.16236877441406\n",
      "Epoca: 9 de 60\n",
      "Loss Train:  284.4698275052584\n",
      "Loss Val:    201.1104278564453\n",
      "Epoca: 10 de 60\n",
      "Loss Train:  282.8697521503155\n",
      "Loss Val:    201.07080078125\n",
      "Epoca: 11 de 60\n",
      "Loss Train:  286.75175006573016\n",
      "Loss Val:    201.04124450683594\n",
      "Epoca: 12 de 60\n",
      "Loss Train:  289.56321598933295\n",
      "Loss Val:    201.0188751220703\n",
      "Epoca: 13 de 60\n",
      "Loss Train:  291.03773146409253\n",
      "Loss Val:    201.00189208984375\n",
      "Epoca: 14 de 60\n",
      "Loss Train:  283.8453369140625\n",
      "Loss Val:    200.98895263671875\n",
      "Epoca: 15 de 60\n",
      "Loss Train:  286.1693514310397\n",
      "Loss Val:    200.9786834716797\n",
      "Epoca: 16 de 60\n",
      "Loss Train:  285.72021836500903\n",
      "Loss Val:    200.97052001953125\n",
      "Epoca: 17 de 60\n",
      "Loss Train:  301.98565086951623\n",
      "Loss Val:    200.9639434814453\n",
      "Epoca: 18 de 60\n",
      "Loss Train:  278.9904339130108\n",
      "Loss Val:    200.9586639404297\n",
      "Epoca: 19 de 60\n",
      "Loss Train:  289.92319547213043\n",
      "Loss Val:    200.95413208007812\n",
      "Epoca: 20 de 60\n",
      "Loss Train:  282.52306189903845\n",
      "Loss Val:    200.9505157470703\n",
      "Epoca: 21 de 60\n",
      "Loss Train:  286.7069584773137\n",
      "Loss Val:    200.9474334716797\n",
      "Epoca: 22 de 60\n",
      "Loss Train:  282.42884826660156\n",
      "Loss Val:    200.9448699951172\n",
      "Epoca: 23 de 60\n",
      "Loss Train:  289.92322598970856\n",
      "Loss Val:    200.9427032470703\n",
      "Epoca: 24 de 60\n",
      "Loss Train:  287.4109156681941\n",
      "Loss Val:    200.94085693359375\n",
      "Epoca: 25 de 60\n",
      "Loss Train:  293.813721876878\n",
      "Loss Val:    200.93923950195312\n",
      "Epoca: 26 de 60\n",
      "Loss Train:  281.21859036959137\n",
      "Loss Val:    200.93788146972656\n",
      "Epoca: 27 de 60\n",
      "Loss Train:  285.8503723144531\n",
      "Loss Val:    200.9366455078125\n",
      "Epoca: 28 de 60\n",
      "Loss Train:  296.8272388164814\n",
      "Loss Val:    200.93556213378906\n",
      "Epoca: 29 de 60\n",
      "Loss Train:  296.0551030085637\n",
      "Loss Val:    200.93460083007812\n",
      "Epoca: 30 de 60\n",
      "Loss Train:  282.22757310133716\n",
      "Loss Val:    200.9337158203125\n",
      "Epoca: 31 de 60\n",
      "Loss Train:  289.3164496788612\n",
      "Loss Val:    200.9329071044922\n",
      "Epoca: 32 de 60\n",
      "Loss Train:  293.79923072228064\n",
      "Loss Val:    200.9322052001953\n",
      "Epoca: 33 de 60\n",
      "Loss Train:  298.8989915114183\n",
      "Loss Val:    200.93153381347656\n",
      "Epoca: 34 de 60\n",
      "Loss Train:  288.50336397611176\n",
      "Loss Val:    200.93092346191406\n",
      "Epoca: 35 de 60\n",
      "Loss Train:  282.6527850811298\n",
      "Loss Val:    200.9303436279297\n",
      "Epoca: 36 de 60\n",
      "Loss Train:  280.5774371807392\n",
      "Loss Val:    200.9298095703125\n",
      "Epoca: 37 de 60\n",
      "Loss Train:  288.26242887056793\n",
      "Loss Val:    200.92929077148438\n",
      "Epoca: 38 de 60\n",
      "Loss Train:  286.9118910569411\n",
      "Loss Val:    200.92880249023438\n",
      "Epoca: 39 de 60\n",
      "Loss Train:  291.3799520639273\n",
      "Loss Val:    200.92835998535156\n",
      "Epoca: 40 de 60\n",
      "Loss Train:  286.5768350454477\n",
      "Loss Val:    200.92791748046875\n",
      "Epoca: 41 de 60\n",
      "Loss Train:  290.44534653883716\n",
      "Loss Val:    200.92750549316406\n",
      "Epoca: 42 de 60\n",
      "Loss Train:  287.89147597092847\n",
      "Loss Val:    200.9270782470703\n",
      "Epoca: 43 de 60\n",
      "Loss Train:  290.65101506159857\n",
      "Loss Val:    200.92666625976562\n",
      "Epoca: 44 de 60\n",
      "Loss Train:  287.5891864483173\n",
      "Loss Val:    200.92628479003906\n",
      "Epoca: 45 de 60\n",
      "Loss Train:  287.5396963266226\n",
      "Loss Val:    200.92591857910156\n",
      "Epoca: 46 de 60\n",
      "Loss Train:  286.0237520658053\n",
      "Loss Val:    200.92552185058594\n",
      "Epoca: 47 de 60\n",
      "Loss Train:  294.5069087101863\n",
      "Loss Val:    200.9251708984375\n",
      "Epoca: 48 de 60\n",
      "Loss Train:  286.9574103722206\n",
      "Loss Val:    200.9248046875\n",
      "Epoca: 49 de 60\n",
      "Loss Train:  290.25344731257513\n",
      "Loss Val:    200.9244384765625\n",
      "Epoca: 50 de 60\n",
      "Loss Train:  282.20692091721753\n",
      "Loss Val:    200.924072265625\n",
      "Epoca: 51 de 60\n",
      "Loss Train:  281.5566840538612\n",
      "Loss Val:    200.92373657226562\n",
      "Epoca: 52 de 60\n",
      "Loss Train:  286.34036020132214\n",
      "Loss Val:    200.9233856201172\n",
      "Epoca: 53 de 60\n",
      "Loss Train:  289.3899160531851\n",
      "Loss Val:    200.9230499267578\n",
      "Epoca: 54 de 60\n",
      "Loss Train:  290.37217712402344\n",
      "Loss Val:    200.92271423339844\n",
      "Epoca: 55 de 60\n",
      "Loss Train:  288.9314234806941\n",
      "Loss Val:    200.92236328125\n",
      "Epoca: 56 de 60\n",
      "Loss Train:  297.4527329664964\n",
      "Loss Val:    200.92205810546875\n",
      "Epoca: 57 de 60\n",
      "Loss Train:  288.51019873985877\n",
      "Loss Val:    200.92173767089844\n",
      "Epoca: 58 de 60\n",
      "Loss Train:  284.6660473163311\n",
      "Loss Val:    200.9214324951172\n",
      "Epoca: 59 de 60\n",
      "Loss Train:  291.7670370248648\n",
      "Loss Val:    200.92111206054688\n",
      "Epoca: 60 de 60\n",
      "Loss Train:  294.623291015625\n",
      "Loss Val:    200.9208221435547\n"
     ]
    }
   ],
   "source": [
    "#Listas donde guardamos loss de entrenamiento, y para el de validación la loss y las métricas de evaluación.\n",
    "RMSE, BIAS, Corr_P, Corr_S = [], [], [], []\n",
    "loss_train, loss_val = [], []\n",
    "\n",
    "#Uno recorre varias veces los mismos datos para acercarse al minimo \n",
    "#recordemos que \n",
    "for epoch in range(max_epochs):\n",
    "    print('Epoca: '+ str(epoch+1) + ' de ' + str(max_epochs) )\n",
    "    \n",
    "    #Entrenamiento del modelo        \n",
    "    model.train()  #Esto le dice al modelo que se comporte en modo entrenamiento.\n",
    "\n",
    "    sum_loss = 0.0\n",
    "    batch_counter = 0\n",
    "\n",
    "    # Iteramos sobre los minibatches. \n",
    "    for inputs, target in dataloader_train :\n",
    "        #Enviamos los datos a la memoria.\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        #-print( 'Batch ' + str(batch_counter) )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "            \n",
    "        loss = MSE_Loss(outputs.float(), target.float())\n",
    "                    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                    \n",
    "        batch_counter += 1\n",
    "        sum_loss = sum_loss + loss.item()\n",
    "\n",
    "    if lr_decay:\n",
    "      scheduler.step()\n",
    "\n",
    "    #Calculamos la loss media sobre todos los minibatches \n",
    "    loss_train.append( sum_loss / batch_counter )\n",
    "\n",
    "    #Calculamos la funcion de costo para la muestra de validacion.\n",
    "    input_val , target_val = next( iter( dataloader_val ) )\n",
    "    input_val , target_val = input_val.detach().to(device) , target_val.detach().to(device) \n",
    "    with torch.no_grad():\n",
    "      output_val = model( input_val )\n",
    "\n",
    "    loss_val.append( MSE_Loss( output_val , target_val ).item() )\n",
    "\n",
    "    #Calculamos la funcion de costo para la muestra de testing.\n",
    "    model.eval()   #Esto le dice al modelo que lo usaremos para evaluarlo (no para entrenamiento)\n",
    "    input_test , target_test = next( iter( dataloader_test ) )\n",
    "    input_test , target_test = input_test.detach().to(device) , target_test.detach().to(device) \n",
    "    #zzprint( input_test.shape )\n",
    "    with torch.no_grad():\n",
    "      output_test = model( input_test )\n",
    "\n",
    "    #Calculo de la loss de la epoca\n",
    "    print('Loss Train: ', str(loss_train[epoch]))\n",
    "    print('Loss Val:   ', str(loss_val[epoch]))\n",
    "\n",
    "    ###################################\n",
    "    np_input_test  = ds.denorm( input_test.numpy()  , dataloader_test.dataset.xmin, dataloader_test.dataset.xmax )\n",
    "    np_target_test = ds.denorm( target_test.numpy() , dataloader_test.dataset.ymin, dataloader_test.dataset.ymax )\n",
    "    np_output_test = ds.denorm( output_test.detach().numpy() , dataloader_test.dataset.ymin, dataloader_test.dataset.ymax )\n",
    "    \n",
    "    np_input_val  = ds.denorm( input_val.numpy()  , dataloader_val.dataset.xmin, dataloader_val.dataset.xmax )\n",
    "    np_target_val = ds.denorm( target_val.numpy() , dataloader_val.dataset.ymin, dataloader_val.dataset.ymax )\n",
    "    np_output_val = ds.denorm( output_val.detach().numpy() , dataloader_val.dataset.ymin, dataloader_val.dataset.ymax )\n",
    "  \n",
    "    #Calculo de metricas RMSE, BIAS, Correlacion de Pearson y Spearman\n",
    "    RMSE.append( ver.rmse( np_output_val , np_target_val ) )\n",
    "    BIAS.append( ver.bias( np_output_val , np_target_val ) )\n",
    "    Corr_P.append( ver.corr_P( np_output_val , np_target_val ) )\n",
    "    Corr_S.append( ver.corr_S( np_output_val , np_target_val ) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQXqUDpl-y9n"
   },
   "source": [
    "### Ploteamos y evaluamos el conjunto de Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "xgtfJmwq-y9n"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/Escritorio/Tesis/Practicas/Semana_2_Miercoles/Convolucion/plots.py:49: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot( bins[0:-1] , np.log( hist_Target) , '-b' , label=Target_name+\" (Target)\" )\n",
      "/home/user/Escritorio/Tesis/Practicas/Semana_2_Miercoles/Convolucion/plots.py:50: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot( bins[0:-1] , np.log( hist_Modelo ) , '-r' , label='Modelo' )\n"
     ]
    }
   ],
   "source": [
    "Muestras = random.sample(range(Data[\"len_test\"]),10)\n",
    "\n",
    "plots.plotting(Directorio_out, np_input_test , np_target_test , np_output_test ,\n",
    "                loss_train, loss_val, \n",
    "                RMSE, BIAS, Corr_P, Corr_S,\n",
    "                Experimento, Input_name, Target_name,\n",
    "                max_epochs, Muestras, nx ,ny)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2vSVq-g-y9o"
   },
   "source": [
    "**<font color='green'>Preguntas</font>**\n",
    "\n",
    "**<font color='green'>Observando la figura de la evolución en función de la época. ¿Cómo fué su evolución? ¿Incurrimos en Underfitting u Overfitting?</font>**\n",
    "\n",
    "**<font color='green'>Discuta la evolución de los scores en función de la época. ¿Sobre cual conjunto se están calculando?</font>**\n",
    "\n",
    "**<font color='green'>Observe similitudes y diferencias entre el target y el output de algunos de los ejemplos de salida graficados. ¿A que se los puede atribuir?</font>**\n",
    "\n",
    "**<font color='green'> </font>**\n",
    "\n",
    "**<font color='green'>Preguntas</font>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "113rR0Hq-y9o"
   },
   "source": [
    "## Guardando el modelo entrenado, y los datos de Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z17nZeJR-y9o"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    torch.save(model.state_dict(), Directorio_out+\"Modelo_exp_\"+str(Numero_exp)+\".pth\")\n",
    "    \n",
    "if False: #Guardar los datos de test\n",
    "    np.savez(Directorio_out+\"Datos_test_\"+Input_name+\"_vs_\"+Target_name+\"_\"+str(Numero_exp)+\".npz\",\n",
    "         Input=input_test.reshape(input_test.shape[0],nx,ny),\n",
    "         Target=target_test.reshape(target_test.shape[0],nx,ny),\n",
    "         Modelo=output_test.reshape(output_test.shape[0],nx,ny),\n",
    "         loss_train = loss_train, loss_val = loss_val,\n",
    "         RMSE = RMSE, BIAS = BIAS, Corr_P = Corr_P, Corr_S = Corr_S,\n",
    "         Experimento = Experimento, Input_name = Input_name, Target_name = Target_name,\n",
    "         max_epochs = max_epochs, nx = nx, ny = ny)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7xSODdS-y9p"
   },
   "source": [
    "### Para levantar los parametros del modelo que ya entrenamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jz_EAGzE-y9p"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    model.load_state_dict(torch.load(Directorio_out+\"Modelo_exp_\"+str(Numero_exp)+\".pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BB3pcNhE-y9q"
   },
   "source": [
    "# Consignas\n",
    "\n",
    "**<font color='red'>Realicemos otras pruebas... Indique números de experimento diferentes para luego comparar (está en la sección de hiperparámetros), anote la configuración utilizada, en particular la inicial para ser de referencia para analizar la sensibilidad.</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4w6TIx9z-y9q"
   },
   "source": [
    "## 1 - Normalización de los datos\n",
    "\n",
    "**<font color='pink'>Inicialmente trabajamos con una normalización del tipo [0,1], ahora elimine la normalización y entrene y comparare el modelo.</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zY4A6yZy-y9r"
   },
   "source": [
    "## 2- Batch Normalization\n",
    "\n",
    "**<font color='pink'>Aplique batch normalization. ¿Que modificó? ¿Cómo fué su desempeño respecto del anterior?</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PC1l6KE-y9r"
   },
   "source": [
    "## 3 - Dropout\n",
    "\n",
    "**<font color='pink'>Ahora activemos el dropout (descomentelo en la sección que construimos el modelo) y setee la configuración de referencia.</font>**\n",
    "\n",
    "- **<font color='green'>Cuando aplique Dropout. ¿Que pasaría si CADA VEZ que ingresa un input, en las capas que se aplique el dropout, cierto porcentaje de neuronas se apagan? Realice la prueba y compare con el experimento de referencia.</font>**\n",
    "\n",
    "Ilustrando:\n",
    "\n",
    "![alternative text](./images_CONV/dropout.webp)\n",
    "\n",
    "Nota: El efecto de dropout solo ocurre durante el entrenamiento de la red, no sobre el testing.\n",
    "\n",
    "![alternative text](./images_CONV/dropout_train_test.webp)\n",
    "\n",
    "- **<font color='green'>Modifique la probabilidad del dropout. ¿Que ocurre si tomamos los casos extremos? Realice la prueba.</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vzhNQb2R-y9r"
   },
   "source": [
    "## 4 - Data Augmentation\n",
    "\n",
    "Es un recurso para aumentar de manera artificial los datos con los cuales entrenamos la red neuronal. Existen diferentes metodologías para llevarlo a cabo, por ejemplo a la imagen original rotarla o añadirle un ruido, que altera suavemente a la imagen, generando una imagen nueva, pero que conserve sus características. En este caso, se deja la opción realizar un data augmentation con rotación alrededor del eje horizontal, vertical y de ambos, de esta forma por cada imagen perteneciente al entrenamiento, se generaran 3 imagenes adicionales, lo cual cuadriplica el tamaño del dataset de entrenamiento.\n",
    "\n",
    "**<font color='green'> ¿Cúal imagen es la real?<font>**\n",
    "\n",
    "![alternative text](./images_CONV/ciclogenesis_flipH_rotate2.png)![alternative text](./images_CONV/ciclogenesis_rotate2.png) ![alternative text](./images_CONV/ciclogenesis_flipHV.png) ![alternative text](./images_CONV/ciclogenesis_flipV.png)\n",
    "\n",
    "![alternative text](./images_CONV/ciclogenesis_rotate1.png)![alternative text](./images_CONV/ciclogenesis_flipH_rotate1.png)![alternative text](./images_CONV/ciclogenesis_flipH.png) ![alternative text](./images_CONV/ciclogenesis.png)\n",
    "\n",
    "**<font color='pink'>Ahora activemos el Data augmentation, setee la configuración de referencia.<font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IsOAvxpu-y9s"
   },
   "source": [
    "## 5 - Más datos\n",
    "\n",
    "**<font color='pink'>Remueva el dropout. Vaya al script de set_dataset, en particular a la función get_data y utilice todos los datos para el entrenamiento. LUEGO REINICIE EL KERNEL y ejecute la notebook. ¿Cómo fué el desempeño del modelo? </font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTjOx19e-y9s"
   },
   "source": [
    "## 6 - Operadores dimensionales en Convolucionales: Maxpooling y Convolución Transpuesta\n",
    "\n",
    "**<font color='pink'>Entre las capas convoluciones con las cuales venimos trabajando, incorpore una capa de Maxpooling y al final de la red una Convolución Transpuesta.</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVp7WsCD-y9t"
   },
   "source": [
    "## 7 - Variando los filtros de la red\n",
    "\n",
    "**<font color='pink'>Aumente o reduzca la cantidad de filtros de las capas convolucionales (no el in inicial y el out final) ¿Qué estamos haciendo? Observe el tiempo de entrenamiento y compare el resultado con la arquitectura inicial.</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c--_PS2J-y9t"
   },
   "source": [
    "## 8 - Variando los argumentos de las convolucionales\n",
    "\n",
    "**<font color='red'>CALCULE LAS DIMENSIONES, utilice la función de abajo que se le da los argumento y calcula la dimensión de salida.</font>**\n",
    "\n",
    "**<font color='pink'>Aumente o reduzca el tamaño de los kernels, stride, padding y dilatation. Observe el tiempo de entrenamiento y compare el resultado con la arquitectura inicial.</font>**\n",
    "\n",
    "**<font color='pink'>También considere variar el modo de padding, busque en la [página](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) los tipos de modos de padding que existen, ¿Qué hacen cada uno?.</font>**\n",
    "\n",
    "**[Página](https://madebyollin.github.io/convnet-calculator/) para calcular dimensiones en convolucionales y maxpooling** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PYZtHgaI-y9t"
   },
   "outputs": [],
   "source": [
    "def conv(size,kernel,stride,padding,dilatation):\n",
    "    \n",
    "    return (size+2*padding-dilatation*(kernel-1)-1)/stride +1\n",
    "\n",
    "def maxpooling(size,kernel,padding,dilatation): #idem convolución, pero para el stride por default es el tamaño del kernel\n",
    "    stride = kernel\n",
    "    return (size+2*padding-dilatation*(kernel-1)-1)/stride +1\n",
    "\n",
    "def transposeConv(size,kernel,stride,padding,dilatation,output_padding):\n",
    "    \n",
    "    return (size-1)*stride-2*padding+dilatation*(kernel-1)+output_padding+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5fhAL_Fw-y9u"
   },
   "outputs": [],
   "source": [
    "print(conv(size = 40, kernel = 11, stride = 1, padding = 5, dilatation = 1))\n",
    "print(maxpooling(size = 40, kernel= 2, padding = 0,dilatation = 1))\n",
    "print(transposeConv(size = 20, kernel = 4, stride = 2, padding = 1,  dilatation = 1, output_padding = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jE12aJIr-y9u"
   },
   "source": [
    "## 9 - Libre\n",
    "\n",
    "**<font color='pink'>Ahora juegue y utilice diferentes pruebas combinando todo lo visto. ¿Cual fué la mejor combinación obtenida?</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2E-hZwk-y9v"
   },
   "source": [
    "# Bibliografía\n",
    "\n",
    "- Deep Learning with Pytorch. Stevens E., Antiga L., & Viehmann T. Manning Publications, 2020.\n",
    "\n",
    "- https://medium.com/mlearning-ai/optimizers-in-deep-learning-7bf81fed78a0\n",
    "\n",
    "- https://medium.com/@monadsblog/dropout-tales-57cf0191f5af"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd206b3f1eb6162bcb3cc8875458ad048b5ae6e6cce8ea55058e9725955ef05a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
